// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview Simulates Gemini API responses using predefined JSON files to populate a threat model.
 *
 * - llmPoweredThreatModelSimulation - A function that simulates Gemini API responses to generate a threat model.
 * - LLMPoweredThreatModelSimulationInput - The input type for the llmPoweredThreatModelSimulation function.
 * - LLMPoweredThreatModelSimulationOutput - The return type for the llmPoweredThreatModelSimulation function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const LLMPoweredThreatModelSimulationInputSchema = z.object({
  systemDescription: z.string().describe('Description of the AI agent system.'),
  maestroLayer: z.string().describe('The MAESTRO layer to generate the threat model for.'),
});
export type LLMPoweredThreatModelSimulationInput = z.infer<
  typeof LLMPoweredThreatModelSimulationInputSchema
>;

const LLMPoweredThreatModelSimulationOutputSchema = z.object({
  threatModel: z.any().describe('The generated threat model as a JSON object.'),
});
export type LLMPoweredThreatModelSimulationOutput = z.infer<
  typeof LLMPoweredThreatModelSimulationOutputSchema
>;

export async function llmPoweredThreatModelSimulation(
  input: LLMPoweredThreatModelSimulationInput
): Promise<LLMPoweredThreatModelSimulationOutput> {
  return llmPoweredThreatModelSimulationFlow(input);
}

const prompt = ai.definePrompt({
  name: 'llmPoweredThreatModelSimulationPrompt',
  input: {schema: LLMPoweredThreatModelSimulationInputSchema},
  output: {schema: LLMPoweredThreatModelSimulationOutputSchema},
  prompt: `You are a threat modeling expert simulating the response of the Gemini API.

You are provided with a system description and a MAESTRO layer.
Based on this information, generate a threat model in JSON format.

System Description: {{{systemDescription}}}
MAESTRO Layer: {{{maestroLayer}}}

Threat Model JSON:
`,
});

const llmPoweredThreatModelSimulationFlow = ai.defineFlow(
  {
    name: 'llmPoweredThreatModelSimulationFlow',
    inputSchema: LLMPoweredThreatModelSimulationInputSchema,
    outputSchema: LLMPoweredThreatModelSimulationOutputSchema,
  },
  async input => {
    await new Promise(resolve => setTimeout(resolve, 1500));
    const simulatedApiResponse = await getSimulatedApiResponse(
      input.maestroLayer
    );
    const threatModel = JSON.parse(simulatedApiResponse);
    return {threatModel};
  }
);

const simulatedResponses: Record<string, string> = {
  'foundation-models': JSON.stringify([
    {
      name: 'Model Inversion Attack',
      description: 'An attacker attempts to reconstruct sensitive training data by querying the model.',
      risk: 'High',
      layer: 'Foundation Models',
    },
    {
      name: 'Membership Inference',
      description: 'An attacker determines if a specific data record was part of the model\'s training set.',
      risk: 'Medium',
      layer: 'Foundation Models',
    }
  ]),
  'data-operations': JSON.stringify([
    {
      name: 'Data Poisoning',
      description: 'An attacker intentionally injects corrupted or misleading data into the training dataset to compromise model behavior.',
      risk: 'High',
      layer: 'Data Operations',
    },
    {
      name: 'Sensitive Data Leakage in Logs',
      description: 'PII or other sensitive information is inadvertently logged during data processing, creating a privacy risk.',
      risk: 'Medium',
      layer: 'Data Operations',
    }
  ]),
  'agent-frameworks': JSON.stringify([
    {
      name: 'Prompt Injection',
      description: 'An attacker crafts inputs that manipulate the agent into ignoring its original instructions and executing the attacker\'s commands.',
      risk: 'Critical',
      layer: 'Agent Frameworks',
    },
    {
      name: 'Unintended Tool Use',
      description: 'The agent misuses an integrated tool (e.g., a file system API or a web browser) in a way that leads to a security vulnerability, such as data exfiltration or remote code execution.',
      risk: 'High',
      layer: 'Agent Frameworks',
    }
  ]),
  'deployment-infrastructure': JSON.stringify([
      {
        name: 'Insecure API Endpoints',
        description: 'The API endpoints that expose the agent are not properly secured, allowing for unauthorized access, rate limit abuse, or denial of service attacks.',
        risk: 'High',
        layer: 'Deployment & Infrastructure',
      },
  ]),
  'default': JSON.stringify([
      {
        name: 'Generic Simulated Threat',
        description: 'This is a simulated threat for the selected layer. A real system description would produce more specific threats.',
        risk: 'Low',
        layer: 'Unknown',
      }
  ])
};


async function getSimulatedApiResponse(
  maestroLayer: string
): Promise<string> {
    const layerKey = maestroLayer.toLowerCase().replace(/ & /g, '-').replace(/ /g, '-');
    const response = simulatedResponses[layerKey] || simulatedResponses['default'];
    // Add layer info to default response
    if (!simulatedResponses[layerKey]) {
        const defaultResponse = JSON.parse(simulatedResponses['default']);
        defaultResponse[0].layer = maestroLayer;
        return JSON.stringify(defaultResponse);
    }
    return response;
}
